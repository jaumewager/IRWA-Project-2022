{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0Wp_pANLs-e",
        "outputId": "74ebc332-45a1-48b6-b388-0cedcba7d5b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IRWA Project"
      ],
      "metadata": {
        "id": "ChJS7A-dyXJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this project is to o build a search engine implementing different indexing and ranking algorithms. This search engine will be developed based on a document corpus composed of a set of tweets related to Hurricane Ian.\n",
        "\n",
        "The project will have the following four incremental steps: Text Processing, Indexing and Evaluation, Ranking, and User Interface and Web Analytics."
      ],
      "metadata": {
        "id": "wVkaevsVz4Py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PART 1: Text Processing"
      ],
      "metadata": {
        "id": "Iiyy1dX2yqn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this first part of the project, we were asked to pre-process the documents (set of tweets) by:\n",
        "\n",
        "- Removing stop words\n",
        "- Tokenization\n",
        "- Removing punnctation marks\n",
        "- Stemming\n",
        "\n",
        "However, we added som pre-processing steps which we thought they would be useful, such as removing the '#' sign from hashtags.\n"
      ],
      "metadata": {
        "id": "0p1LG5S-1RUx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMClweL6Vna8"
      },
      "source": [
        "#### Load Python packages\n",
        "We will first import all the packages that we will use during this first part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "id": "cTy7I3R0Vna-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f258a6bc-e6d0-4cbd-9766-7f061dc8cc85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
        "import nltk\n",
        "import time\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "F3-qwPE1VnbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06583af3-8059-4228-b92e-937b2df2e676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting language-data\n",
            "  Downloading language_data-1.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 7.6 MB/s \n",
            "\u001b[?25hCollecting marisa-trie<0.8.0,>=0.7.7\n",
            "  Downloading marisa_trie-0.7.7-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 48.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from marisa-trie<0.8.0,>=0.7.7->language-data) (57.4.0)\n",
            "Installing collected packages: marisa-trie, language-data\n",
            "Successfully installed language-data-1.1 marisa-trie-0.7.7\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "from array import array\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import math\n",
        "import numpy as np\n",
        "import collections\n",
        "import json\n",
        "import pandas as pd\n",
        "from numpy import linalg as la\n",
        "!pip install language-data\n",
        "from langcodes import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqPgQGeEVnbC"
      },
      "source": [
        "#### Load data into memory\n",
        "The document corpus is stored in a JSON file, and as we already mentioned, it is composed by a set of tweets related to Hurricane Ian. We have a lot of information for each tweet, which we will have to select the relevant and useful one for future stages."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opening the json file as plaintext, and loading each tweet to a list of tweets."
      ],
      "metadata": {
        "id": "EVr6dZFSMemP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ln0JR46XVnbD"
      },
      "outputs": [],
      "source": [
        "docs_path = '/content/drive/Shareddrives/RIAW/data_part1/tw_hurricane_data.json'\n",
        "info_needed=[\"created_at\",\"tweet_id\",\"full_text\",\"username\",\"favorite_count\", \"retweet_count\",\"hashtags\",\"URL\"]\n",
        "tweets = [] #list of tweets but as json objects\n",
        "for line in open(docs_path, 'r'):\n",
        "    tweets.append(json.loads(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Define useful functions"
      ],
      "metadata": {
        "id": "dSkl9sqU5aJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the function ```extract_hashtags(tweet)```.\n",
        "\n",
        "It takes as input a tweet and extract its hashtags into a hashtags array."
      ],
      "metadata": {
        "id": "c7F9ZNsX5sxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hashtags(tweet):\n",
        "  hashtags=[]\n",
        "  for i in range (len(tweet[\"entities\"][\"hashtags\"])):\n",
        "    hashtags.append(tweet[\"entities\"][\"hashtags\"][i][\"text\"])\n",
        "  return hashtags\n"
      ],
      "metadata": {
        "id": "uUOp6c4bSUb-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the function ```extract_username(tweet)```.\n",
        "\n",
        "It takes as input a tweet and extract its username (we chose the screen_name since it does not have special characters nor emojis) into a hashtags array."
      ],
      "metadata": {
        "id": "9NLh8J3s8QFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_username(tweet):\n",
        "  return tweet[\"user\"][\"screen_name\"]"
      ],
      "metadata": {
        "id": "FYx6EBV6Xeax"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the function ```extract_URL(tweet)```.\n",
        "\n",
        "It takes as input a tweet and return its URL in case they have. Otherwise, it returns a \"NO URL\" tag identifying the missing value."
      ],
      "metadata": {
        "id": "vkVeYSLz8QwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_URL(tweet):\n",
        "  try:\n",
        "    return tweet[\"entities\"][\"urls\"][0][\"url\"]\n",
        "  except:\n",
        "    return \"NO URL\""
      ],
      "metadata": {
        "id": "o116cojcZf7Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the function ```get_stopwords(tweet)```.\n",
        "\n",
        "It takes as input a set of tweets and it returns all stopwords from the different languages of the tweets. In this case the tweets are all in english, but might be useful for future files of tweets"
      ],
      "metadata": {
        "id": "yNCfxsx7OchQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stopwords(tweets):\n",
        "  languages=[]\n",
        "  stop_words=set(languages)\n",
        "  for tweet in tweets:\n",
        "    lang=Language.get(tweet['lang']).display_name().lower()\n",
        "    if lang not in languages:\n",
        "      languages.append(lang)\n",
        "  for lang in languages:\n",
        "    stop_words = stop_words.union(set(stopwords.words(lang)))\n",
        "  return stop_words"
      ],
      "metadata": {
        "id": "3rk4LiyKMO0v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the function ```eliminate_hashtags_and_urls(line)```.\n",
        "\n",
        "It takes as input a text (the actual tweet) and returns an array of words without including hashtags and urls."
      ],
      "metadata": {
        "id": "6Ni9uXZB9zME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminate_hashtags_and_urls(line):\n",
        "  words=[]\n",
        "  for word in line:\n",
        "    if not(word.__contains__(\"#\") or word.__contains__(\"http\") or not(word.isalnum())):\n",
        "      words.append(word)\n",
        "  return words"
      ],
      "metadata": {
        "id": "SJcio0UgzXqu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u5jf_JhVnbF"
      },
      "source": [
        "Implement the function ```build_terms(line)```.\n",
        "\n",
        "It takes as input a text and performs the following operations:\n",
        "\n",
        "- Transform all text to lowercase\n",
        "- Tokenize the text to get a list of terms (use *split function*)\n",
        "- Eliminate the hashtags and urls using the previously defined function\n",
        "- Remove stop words\n",
        "- Stem terms \n",
        "- Replace the elements present in *elements_to_replace* by a blank\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fQklp6L1VnbG"
      },
      "outputs": [],
      "source": [
        "def build_terms(line,stopwords):\n",
        "    \n",
        "    elements_to_replace=['}','{','[',']','\"',',']\n",
        "    stemmer = PorterStemmer()\n",
        "    stop_words = stopwords\n",
        "    \n",
        "    line=  line.lower() ## Transform in lowercase\n",
        "    line=  line.split(\" \") ## Tokenize the text to get a list of terms\n",
        "    line = eliminate_hashtags_and_urls(line)\n",
        "    line = [x for x in line if x not in stop_words]  ##eliminate the stopwords\n",
        "    line = [stemmer.stem(word) for word in line] ## perform stemming\n",
        "    for e in elements_to_replace:\n",
        "      line = [word.strip().replace(e, '')for word in line]\n",
        "   \n",
        "    return line"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, we have worked with the *tweet_document_ids_map.csv* file. The code below reads the file, eliminates the \\t and \\n terms, splits the doc_id from the tweet_id, creating an array, and then it converts the tweet_id to integer. Finally, each array created is added to the doc_ids matrix."
      ],
      "metadata": {
        "id": "grZcHlY1-w8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_doc = '/content/drive/Shareddrives/RIAW/data_part1/tweet_document_ids_map.csv'\n",
        "with open(mapping_doc) as fp:\n",
        "    initial_docs_ids = fp.readlines()\n",
        "    docs_ids=[]\n",
        "    for doc_id in initial_docs_ids:\n",
        "      doc_id = ' '.join(doc_id.split())\n",
        "      doc_id = doc_id.split()\n",
        "      doc_id[1] = int(doc_id[1])\n",
        "      docs_ids.append(doc_id)"
      ],
      "metadata": {
        "id": "tSD5jTQ62DO-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the function ```define_key(tweet, docs_ids)```.\n",
        "\n",
        "It takes as input a tweet and an array of docs_ids, and it returns the document id of the tweet."
      ],
      "metadata": {
        "id": "1IdJ9t44__j7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_key(tweet,docs_ids):\n",
        "  for id in docs_ids:\n",
        "    if tweet[\"id\"]==id[1]:\n",
        "      return id[0]"
      ],
      "metadata": {
        "id": "PcG8_12o_Xk-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the function ```preprocess_tweets(tweets, info_needed, doc_ids)```.\n",
        "\n",
        "It takes as input the list of tweets, the info_needed (tweet's information we want to extract to work on in the future) and an array of doc_ids. The function performs the following operations:\n",
        "\n",
        "- Gets the stopwords of the tweets with the ```get_stopwords(tweet)``` function\n",
        "\n",
        "And then for each tweet:\n",
        "- Define a new key with the ```define_key(tweet,doc_ids)``` function\n",
        "\n",
        "And for each information needed of the tweet:\n",
        "- Apply to the tweet the different extracting functions defined at the beginning or extract directly the information in some cases and save them to the processed_tweet dictionary\n",
        "- Mapping the document id (key) defined previously with its corresponding dictionary of values (tweet information), and adding it to the dictionary created beforewards (tweets_processed)\n",
        "\n",
        "Finally the function returns the dictionary of dictionaries: tweets_processed."
      ],
      "metadata": {
        "id": "ZBPn46REAgf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweets(tweets,info_needed, doc_ids):\n",
        "  stopwords=get_stopwords(tweets)\n",
        "  tweets_processed={}\n",
        "  for i in range (len(tweets)):\n",
        "    tweet=tweets[i]\n",
        "    new_key=define_key(tweet,doc_ids)\n",
        "    processed_tweet={}\n",
        "    for j in range(len(info_needed)):\n",
        "      if info_needed[j]==\"created_at\":\n",
        "          processed_tweet[info_needed[j]]=tweet[\"created_at\"]\n",
        "      if info_needed[j]==\"tweet_id\":\n",
        "          processed_tweet[info_needed[j]]=tweet[\"id\"]\n",
        "      if info_needed[j]==\"full_text\":\n",
        "          processed_tweet[info_needed[j]]=build_terms(tweet[\"full_text\"],stopwords)\n",
        "      if info_needed[j]==\"username\":\n",
        "          processed_tweet[info_needed[j]]=extract_username(tweet)\n",
        "      if info_needed[j]==\"favorite_count\":\n",
        "          processed_tweet[info_needed[j]]=tweet[\"favorite_count\"]\n",
        "      if info_needed[j]==\"retweet_count\":\n",
        "          processed_tweet[info_needed[j]]=tweet[\"retweet_count\"]\n",
        "      if info_needed[j]==\"hashtags\":\n",
        "          processed_tweet[info_needed[j]]=extract_hashtags(tweet)\n",
        "      if info_needed[j]==\"URL\":\n",
        "          processed_tweet[info_needed[j]]=extract_URL(tweet)\n",
        "    tweets_processed[new_key]=processed_tweet\n",
        "  return tweets_processed"
      ],
      "metadata": {
        "id": "Sao7C5ykEnh5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the ```preprocess_tweets(tweets, info_needed, doc_ids)``` function to all the set of tweets forming the document corpus."
      ],
      "metadata": {
        "id": "MmaANDaFKm31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_tweets = preprocess_tweets(tweets, info_needed,docs_ids)"
      ],
      "metadata": {
        "id": "JKMScO9c-J2H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we create a table with pandas.DataFrame of the dictionary of dictionaries obtained in the previous cell and transpose it so we have a more visual and clear table."
      ],
      "metadata": {
        "id": "RJIzlzM-LvKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict(preprocessed_tweets)\n",
        "df = df.transpose()\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "sC8pBwIh7VkF",
        "outputId": "413bce9b-68a5-4c3b-aa37-52e334a805af"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              created_at             tweet_id  \\\n",
              "doc_1     Fri Sep 30 18:39:08 +0000 2022  1575918182698979328   \n",
              "doc_2     Fri Sep 30 18:39:01 +0000 2022  1575918151862304768   \n",
              "doc_3     Fri Sep 30 18:38:58 +0000 2022  1575918140839673873   \n",
              "doc_4     Fri Sep 30 18:38:57 +0000 2022  1575918135009738752   \n",
              "doc_5     Fri Sep 30 18:38:53 +0000 2022  1575918119251419136   \n",
              "...                                  ...                  ...   \n",
              "doc_3996  Fri Sep 30 14:33:06 +0000 2022  1575856268022992896   \n",
              "doc_3997  Fri Sep 30 14:33:01 +0000 2022  1575856245650919424   \n",
              "doc_3998  Fri Sep 30 14:32:57 +0000 2022  1575856228886089728   \n",
              "doc_3999  Fri Sep 30 14:32:56 +0000 2022  1575856226139017216   \n",
              "doc_4000  Fri Sep 30 14:32:56 +0000 2022  1575856225908326400   \n",
              "\n",
              "                                                  full_text         username  \\\n",
              "doc_1                             [keep, spin, us, 7, away]         suzjdean   \n",
              "doc_2     [heart, go, affect, wish, everyon, road, curre...             lytx   \n",
              "doc_3                    [kissimme, neighborhood, michigan]       CHeathWFTV   \n",
              "doc_4     [one, tree, backyard, scare, poltergeist, tree...      spiralgypsy   \n",
              "doc_5     [pray, everyon, affect, associ, sympathi, anim...       Blondie610   \n",
              "...                                                     ...              ...   \n",
              "doc_3996  [carrboro, public, servic, place, stand, best,...     CarrboroFire   \n",
              "doc_3997       [list, widespread, flood, bc, bc, low, even]    Baconbitsnews   \n",
              "doc_3998                             [realli, flood, flute]         jganyfl1   \n",
              "doc_3999          [damag, area, punta, tropic, gulf, power]        haddad_cj   \n",
              "doc_4000  [busi, scheme, traffick, asylum, seeker, prope...  Ohemgeeitsalys1   \n",
              "\n",
              "         favorite_count retweet_count  \\\n",
              "doc_1                 0             0   \n",
              "doc_2                 0             0   \n",
              "doc_3                 0             0   \n",
              "doc_4                 0             0   \n",
              "doc_5                 0             0   \n",
              "...                 ...           ...   \n",
              "doc_3996              2             0   \n",
              "doc_3997              0             0   \n",
              "doc_3998             16             8   \n",
              "doc_3999              2             1   \n",
              "doc_4000              0             0   \n",
              "\n",
              "                                                   hashtags  \\\n",
              "doc_1                                        [HurricaneIan]   \n",
              "doc_2                                        [HurricaneIan]   \n",
              "doc_3                                        [HurricaneIan]   \n",
              "doc_4                                  [scwx, HurricaneIan]   \n",
              "doc_5                                        [HurricaneIan]   \n",
              "...                                                     ...   \n",
              "doc_3996                 [CarrboroSafe, ncwx, HurricaneIan]   \n",
              "doc_3997  [Kissimmee, SaintCloud, BlueCounty, Disney, De...   \n",
              "doc_3998                 [HurricaneIan, Florida, MAGATears]   \n",
              "doc_3999                                     [HurricaneIan]   \n",
              "doc_4000                           [DeSantis, HurricaneIan]   \n",
              "\n",
              "                              URL  \n",
              "doc_1                      NO URL  \n",
              "doc_2                      NO URL  \n",
              "doc_3                      NO URL  \n",
              "doc_4                      NO URL  \n",
              "doc_5                      NO URL  \n",
              "...                           ...  \n",
              "doc_3996  https://t.co/jrmrS3tJXa  \n",
              "doc_3997  https://t.co/JaOkK6skP9  \n",
              "doc_3998                   NO URL  \n",
              "doc_3999                   NO URL  \n",
              "doc_4000  https://t.co/BzhXaAPv7B  \n",
              "\n",
              "[4000 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51f3da5d-c888-4bce-9d24-5ff98221a506\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>username</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>doc_1</th>\n",
              "      <td>Fri Sep 30 18:39:08 +0000 2022</td>\n",
              "      <td>1575918182698979328</td>\n",
              "      <td>[keep, spin, us, 7, away]</td>\n",
              "      <td>suzjdean</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[HurricaneIan]</td>\n",
              "      <td>NO URL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_2</th>\n",
              "      <td>Fri Sep 30 18:39:01 +0000 2022</td>\n",
              "      <td>1575918151862304768</td>\n",
              "      <td>[heart, go, affect, wish, everyon, road, curre...</td>\n",
              "      <td>lytx</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[HurricaneIan]</td>\n",
              "      <td>NO URL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_3</th>\n",
              "      <td>Fri Sep 30 18:38:58 +0000 2022</td>\n",
              "      <td>1575918140839673873</td>\n",
              "      <td>[kissimme, neighborhood, michigan]</td>\n",
              "      <td>CHeathWFTV</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[HurricaneIan]</td>\n",
              "      <td>NO URL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_4</th>\n",
              "      <td>Fri Sep 30 18:38:57 +0000 2022</td>\n",
              "      <td>1575918135009738752</td>\n",
              "      <td>[one, tree, backyard, scare, poltergeist, tree...</td>\n",
              "      <td>spiralgypsy</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[scwx, HurricaneIan]</td>\n",
              "      <td>NO URL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_5</th>\n",
              "      <td>Fri Sep 30 18:38:53 +0000 2022</td>\n",
              "      <td>1575918119251419136</td>\n",
              "      <td>[pray, everyon, affect, associ, sympathi, anim...</td>\n",
              "      <td>Blondie610</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[HurricaneIan]</td>\n",
              "      <td>NO URL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_3996</th>\n",
              "      <td>Fri Sep 30 14:33:06 +0000 2022</td>\n",
              "      <td>1575856268022992896</td>\n",
              "      <td>[carrboro, public, servic, place, stand, best,...</td>\n",
              "      <td>CarrboroFire</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>[CarrboroSafe, ncwx, HurricaneIan]</td>\n",
              "      <td>https://t.co/jrmrS3tJXa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_3997</th>\n",
              "      <td>Fri Sep 30 14:33:01 +0000 2022</td>\n",
              "      <td>1575856245650919424</td>\n",
              "      <td>[list, widespread, flood, bc, bc, low, even]</td>\n",
              "      <td>Baconbitsnews</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Kissimmee, SaintCloud, BlueCounty, Disney, De...</td>\n",
              "      <td>https://t.co/JaOkK6skP9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_3998</th>\n",
              "      <td>Fri Sep 30 14:32:57 +0000 2022</td>\n",
              "      <td>1575856228886089728</td>\n",
              "      <td>[realli, flood, flute]</td>\n",
              "      <td>jganyfl1</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>[HurricaneIan, Florida, MAGATears]</td>\n",
              "      <td>NO URL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_3999</th>\n",
              "      <td>Fri Sep 30 14:32:56 +0000 2022</td>\n",
              "      <td>1575856226139017216</td>\n",
              "      <td>[damag, area, punta, tropic, gulf, power]</td>\n",
              "      <td>haddad_cj</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>[HurricaneIan]</td>\n",
              "      <td>NO URL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_4000</th>\n",
              "      <td>Fri Sep 30 14:32:56 +0000 2022</td>\n",
              "      <td>1575856225908326400</td>\n",
              "      <td>[busi, scheme, traffick, asylum, seeker, prope...</td>\n",
              "      <td>Ohemgeeitsalys1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[DeSantis, HurricaneIan]</td>\n",
              "      <td>https://t.co/BzhXaAPv7B</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4000 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51f3da5d-c888-4bce-9d24-5ff98221a506')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51f3da5d-c888-4bce-9d24-5ff98221a506 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51f3da5d-c888-4bce-9d24-5ff98221a506');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}